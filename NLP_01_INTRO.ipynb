{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyJar6MZKfm6DrEF5S3Cr9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Yxaif2sYn3"
      },
      "source": [
        "# **자연어 처리 가이드(Day 01)**\n",
        "***\n",
        "\n",
        "## **자연어 처리 개요**\n",
        "\n",
        "#### 자연어 처리에는 정말 다양한 Task가 있다. 텍스트 분류, 텍스트 유사도, 텍스트 생성, 기계 번역 등 많은 분야가 존재하는데 각 task에 맞게 데이터를 정제하고 모델을 구축하는 것이 중요합니다.\n",
        "\n",
        "### **1. 단어 표현**\n",
        "\n",
        "#### 자연어 처리에서 데이터는 비정형 데이터로 대부분 문자열 형태로 되어있습니다. 따라서 분석을 진행할 때 모델 즉, 컴퓨터가 텍스트를 인식할 수 있는 형태로 변환해주어야 합니다. 이를 위해 보통 '유니코드' 방법을 통해서 텍스트를 인식하게 되는데 이러한 경우에는 언어적인 특성이 반영되지 않아 텍스트를 수치로 변환하는 다른 방법이 필요합니다. 따라서 자연어 처리에서는 보통 텍스트를 벡터로 표현합니다.\n",
        "\n",
        "#### 텍스트를 표현하는 가장 기본은 '원-핫 인코딩' 방식 입니다. 해당 방식은 값을 0과 1만 가지는 이진 형태의 벡터로 변환하는 것으로 값 중에서 1은 실제 값과 매칭되는 부분을 의미하고 0은 매칭되지 않는 부분을 의미합니다. 아래의 간단한 예시를 들어보겠습니다.\n",
        "\n",
        "- 예를 들어 '사과', '귤', '오렌지'를 원-핫 인코딩 벡터로 표현하게 되면 총 3개의 차원에서 사과는 [1, 0, 0], 귤은 [0, 1, 0], 오렌지는 [0, 0, 1]을 의미하게 됩니다.\n",
        "\n",
        "#### 즉, 원-핫 인코딩은 각 단어의 인덱스를 지정하고 단어 벡터에서 단어에 해당하는 인덱스 값만을 1로 표현하는 방식입니다. 매우 직관적이고 이해하기 쉽다는 장점이 있지만 큰 문제점이 2가지가 존재합니다.\n",
        "\n",
        "- 실제로 자연어 처리 관련 분석을 할 경우 단어가 매우 많기 때문에 단어 벡터의 크기가 커지지만 그중에서 1의 값은 단 1개이기 때문에 매우 비효율적입니다.\n",
        "- 단어를 표현하기만 할 뿐 해당 단어가 가지고 있는 의미와 특성은 반영되지 못합니다.\n",
        "\n",
        "#### 이런 문제들을 해결하기 위해서 다른 방식들이 제안되었는데 일반적으로 **분포 가설**(Distributed hypothesis)을 기반으로 합니다. 분포 가설이란 '같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다.'라는 개념입니다. \n",
        "\n",
        "#### 분포 가설 활용법은 크게 두 가지로 나뉘는데 특정 문맥 안에서 단어들의 동시 등장 횟수를 세는 **카운트 기반** 방법과 신경망 등을 통해서 문맥 안의 단어를 예측하는 **예측** 방법이 있습니다.\n",
        "<br>\n",
        "\n",
        "#### **< 카운트 기반 방법 >**\n",
        "\n",
        "#### 카운트 기반 방법은 특정 글의 문맥 안에서 단어가 동시에 등장하는 횟수를 세는 방법입니다. 동시 등장 횟수를 행렬로 나타낸뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법으로 아래와 같은 방식들이 있습니다.\n",
        "\n",
        "- 특이값 분해(SVD)\n",
        "- 잠재의미분석(LSA)\n",
        "- HAL(Hyperspace Analogue to Language)\n",
        "- Hellinger PCA\n",
        "\n",
        "#### 위의 방법은 단어 동시 출현 행렬을 만들고 벡터로 변형하는 방식입니다. 아래 예시를 살펴보도록 하겠습니다.\n",
        "\n",
        "- 철수는 영희와 밥을 먹었다.\n",
        "- 영희와 민수는 어제 공부를 했다.\n",
        "\n",
        "#### 위 텍스트를 동시 출현 행렬로 바꾸면 아래 테이블과 같습니다.\n",
        "\n",
        "\n",
        "| 기준 | 철수는 | 영희와 | 밥을 | 먹었다. | 민수는 | 어제 | 공부를 | 했다. |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "|철수는|0|1|1|1|0|0|0|0|\n",
        "|영희와 |1|0|1|1|1|1|1|1|\n",
        "|밥을 |1|1|0|1|0|0|0|0|\n",
        "| 먹었다.|1|1|1|0|0|0|0|0|\n",
        "|민수는|0|1|0|0|0|1|1|1|\n",
        "|어제|0|0|0|0|1|1|0|1|1|\n",
        "|공부를|0|0|0|0|1|1|1|0|1|\n",
        "|했다.|0|0|0|0|1|1|1|0|\n",
        "\n",
        "#### 이러한 카운트 기반 방법의 장점은 빠르다는 점입니다. 적은 시간으로 단어 벡터를 빠르게 만들 수 있다는 효율성이 있어 아직까지도 많이 사용되는 방법입니다.\n",
        "<br>\n",
        "\n",
        "#### **< 예측 방법 >**\n",
        "\n",
        "#### 예측 방법은 신경망 구조 혹은 어떤 모델을 사용하여 특정 문맥에서 어떤 단어가 등장할지 예측하면서 단어 벡터를 만드는 방식으로 아래와 같은 방식이 있습니다.\n",
        "\n",
        "- Word2vec\n",
        "- NNLM(Neural Network Language Model)\n",
        "- RNNLM(Recurrent Neural Network Language Model)\n",
        "\n",
        "#### 이중에서 Word2vec은 **CBOW(Continuous Bag of Words)**와 **SKip-gram**이라는 두 가지 모델로 나뉩니다. 두 모델은 각각 상반되는 개념으로 CBOW는 주변 단어들을 통해 타겟 단어를 예측하는 방식이며 Skip-gram은 한 단어를 가지고 주변 타겟 단어들을 예측하는 방식입니다. 아래 예시를 살펴보도록 하죠.\n",
        "\n",
        "- '민수가 가방에서 책을 꺼냈다.'라는 문장이 있을 때 \n",
        "- CBOW : 민수가 가방에서 ___ 꺼냈다.\n",
        "- Skip-gram : ____ ____ 책을 ____.\n",
        "\n",
        "#### 두 모델은 학습이 끝난 후 가중치 행렬(신경망의 목표)의 강 행을 단어 벡터로 사용합니다. 이 때 각 단어 벡터간의 유사도를 측정할 수 있어 복잡한 특징 또한 잘 잡아낼 수 있습니다.\n",
        "\n",
        "#### 간단히 예를 들어 단어 벡터상에서 '엄마'와 '아빠'라는 단어의 거리는 '남자'와 '여자' 단어의 벡터 사이의 거리가 같게 나올수 있다는 의미입니다. \n",
        "\n",
        "#### 일반적으로는 Skip-gram이 CBOW 보다 성능이 좋아 자주 사용됩니다. 카운트 기반 방법과 예측 기반 방법 중에서는 예측 기반 방법이 성능이 더 뛰어나 예측 기반 방법이 사용됩니다. \n",
        "\n",
        "#### 단어 표현은 모든 자연어 처리 문제의 기반이 되는 가장 기본적인 내용이라고 할 수 있습니다. "
      ]
    }
  ]
}